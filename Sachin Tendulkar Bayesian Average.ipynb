{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc825bde",
   "metadata": {},
   "source": [
    "# Overview\n",
    "Inspired by this [reddit](https://www.reddit.com/r/Cricket/comments/647njn/bayesian_analysis_of_steve_smiths_average_over/) post where the bayesian average is calculated for different test match batters. The bayesian average of a batter attempts to solve the issues of the classical average used for batters. The formula to work out the classical average is the $\\frac{\\text{no. runs scored}}{\\text{dismissals}}$. Suppose Player1 after 2 innings has scores of 100, 10 resulting in an average of 55. Would this player be better than Player2 who averaged 47 after 50 innings. \n",
    "\n",
    "If you were looking purely at their averages you would say so, highlighting the two main issues with the classical average: it doesn't take into account the variance in the scores or the longevity of their career. Using the bayesian method allows us to evaluate the distribution of a players scores and see how much better/worse they are compared to an average players distribution. The prior belief, based of the reddit post, is that a test match batter averages is normally distributed with a mean of 39.15 and a standard deviation 8.78. The prior is then updated with the new average after each dismissal resulting in the new posterior, which also has a normal distribution. \n",
    "\n",
    "The data is gathered from [Howstat](http://www.howstat.com/cricket/statistics/players/PlayerProgressBat.asp?PlayerID=3463) and the requests and BeautifulSoup libraries are used to scrape the data into a Pandas dataframe. The technique used to scrape the data from the website was found from this [Medium](https://medium.com/geekculture/web-scraping-tables-in-python-using-beautiful-soup-8bbc31c5803e) article. Relevant columns are extracted, cleaned and from that variance and standard error are calculated. \n",
    "\n",
    "The formulas to calculate the posterior mean and standard deviations (shown later) were found in the 'Bayesian Data Analysis' textbook. The new posterior values are calculated in a for loop. The distributions for the prior, likelihood and posterior are plotted using seaborn and are then animated to see how it changes with each dismissal.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f748ae",
   "metadata": {},
   "source": [
    "## Techniques \n",
    "- Used requests and BeautifulSoup libraries to scrape data from website and turn into a Pandas dataframe\n",
    "- Cleaned dataframe removing missing/eroneous values \n",
    "- Performed feature engineering to generate relevant data needed for analysis\n",
    "- Implemented a bayesian analysis of a normal model with multiple observations\n",
    "- Matplotlib and Seaborn libraries used to create visualisation of data\n",
    "- Arviz used to calculate the 90% HDI of the posterior distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f6f502",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib import animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999f101c",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://www.howstat.com/cricket/Statistics/Players/PlayerProgressBat.asp?PlayerID=1735'\n",
    "r = requests.get(url)\n",
    "r.ok"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c68859",
   "metadata": {},
   "source": [
    "Will now use BeautifulSoup to extract the table from the website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94007129",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "table = soup.find('table',{'class':'TableLined'})\n",
    "print(type(table))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19aa5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = table.find_all('tr') # finding all the tr tags(rows)\n",
    "print(len(rows))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ac2651",
   "metadata": {},
   "source": [
    "First created an empty dataframe with the correct column headings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1df94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns = [\"Match\",\"Innings\",\"Date\",\"Versus\", \"Ground\",\"Inns\",\"M/Inns\", \"Method_dismissal\",\"Runs\",\"B/F\",\"Inns_S/R\", \"gap\" ,\"Aggr_Runs\",\"Avg\",\"S/R\"])\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec4db19",
   "metadata": {},
   "source": [
    "Iterated through each row and assigned the correct value to the correct column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ffd8e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(3, len(rows) - 1):\n",
    "    columns = rows[i].find_all('td')\n",
    "    if (columns != []):\n",
    "        match = columns[0].text.strip()\n",
    "        innings= columns[1].text.strip()\n",
    "        date = columns[2].text.strip()\n",
    "        vs = columns[3].text.strip()\n",
    "        grnd = columns[4].text.strip()\n",
    "        inns = columns[5].text.strip()\n",
    "        m_inns = columns[6].text.strip()\n",
    "        m_dismissal = columns[7].text.strip()\n",
    "        runs = columns[8].text.strip()\n",
    "        balls_faced = columns[9].text.strip()\n",
    "        inn_sr = columns[10].text.strip()\n",
    "        gap = columns[11].text.strip()\n",
    "        aggr_runs = columns[12].text.strip()\n",
    "        avg = columns[13].text.strip()\n",
    "        sr = columns[14].text.strip()\n",
    "        \n",
    "        df = df.append({'Match':match,'Innings':innings,'Date':date,\"Versus\":vs,\"Ground\":grnd,\"Inns\":inns,\"M/Inns\":m_inns,\"Method_dismissal\":m_dismissal, \"Runs\":runs, \"B/F\":balls_faced,\"Inns_S/R\":inn_sr,\"gap\":gap, \"Aggr_Runs\":aggr_runs,\"Avg\":avg, \"S/R\":sr}, ignore_index = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a338312d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d04371a",
   "metadata": {},
   "source": [
    "Can now begin cleaning data e.g removing unecessary columns such as match, date, versus, ground, inns, M/Inns, B/F, gap, S/R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9054b1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_columns = [\"Match\", \"Date\",\"Versus\",\"Ground\",\"Inns\",\"M/Inns\", \"B/F\", \"Inns_S/R\",\"gap\",\"S/R\"]\n",
    "\n",
    "player = df.drop(columns = drop_columns, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45daeb45",
   "metadata": {},
   "source": [
    "Will also remove rows where players method of dismissal was 'did not bat' as it doesn't contribute valuable information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ddedd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_1 = player[\"Method_dismissal\"] == 'did not bat'\n",
    "player_clean = player.drop(player.index[mask_1], axis = 0)\n",
    "mask_2 = player_clean[\"Method_dismissal\"] == 'absent hurt'\n",
    "player_clean = player_clean.drop(player_clean.index[mask_2], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d4b5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "player_stats = player_clean.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4a1bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def not_out(x):\n",
    "    '''Function takes in an array, x, and returns numpy array, arr. It determines the number of not out's a player\n",
    "    has at the i'th innings '''\n",
    "    count = int(0)\n",
    "    size = len(x)\n",
    "    arr = np.zeros(size)\n",
    "    for index, item in enumerate(x):\n",
    "        if item == 'not out':\n",
    "            count += 1\n",
    "            arr[index] = count\n",
    "        else:\n",
    "            arr[index] = count\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4120f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "player_stats[\"NO\"] = not_out(player_stats[\"Method_dismissal\"]).astype('int32')# applying function to calculate the number of not outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52fdca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "player_stats['Innings'] = player_stats['Innings'].astype('int32') # converting innings to integer from string to find the number of dismissals\n",
    "player_stats[\"Dismissal\"] = player_stats['Innings'] - player_stats[\"NO\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972f102c",
   "metadata": {},
   "outputs": [],
   "source": [
    "player_stats['Runs_Clean'] = player_stats['Runs'].str.strip('*') # Removing * from the runs column leaving just the number\n",
    "player_stats['Runs_Clean'] = player_stats['Runs_Clean'].astype('int32') # converting data type to integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f830012",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_col_drop = [\"Method_dismissal\", \"Runs\"] # can now remove these columns as they are no longer necessary\n",
    "player_nums = player_stats.drop(columns = new_col_drop, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0511c09",
   "metadata": {},
   "source": [
    "## Calculating Relevant Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee377d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def runs_var(x):\n",
    "    '''Function takes in an array, x, and returns numpy array, var_arr. It determines the variance from innings 0 to innings i. '''\n",
    "    size = len(x)\n",
    "    var_arr = np.zeros(size)\n",
    "    for i in range(size):\n",
    "        var_arr[i] = np.var(player_nums.loc[0:i,'Runs_Clean'])\n",
    "    return var_arr "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73b368c",
   "metadata": {},
   "outputs": [],
   "source": [
    "player_nums[\"var\"] = runs_var(player_nums['Runs_Clean']) # applying function to calculate variance from the first to the i'th innings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95186e4b",
   "metadata": {},
   "source": [
    "As we are not interested in the not out scores will drop the rows where the dismissal doesn't increase by 1 each time. Will also drop the 1st row where variance is equal to 0 this is isn't useful information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c438e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "player_drop_dup = player_nums.drop_duplicates(\"Dismissal\", keep = 'first')\n",
    "mask = player_drop_dup[\"var\"] == 0\n",
    "stats = player_drop_dup.drop(player_drop_dup.index[mask], axis = 0)\n",
    "stats = stats.reset_index(drop = True) # reset index for ease of indentifying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8f30c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# can now calculate the standard error\n",
    "stats[\"std_err\"] = (stats[\"var\"] / stats[\"Dismissal\"]) ** 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45cf8d39",
   "metadata": {},
   "source": [
    "### Visualisation of Runs Scored"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1c2c5d",
   "metadata": {},
   "source": [
    "Graph below shows the runs scored per innings over a players career"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a236a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "sns.lineplot(data = player_nums, x = 'Innings', y = 'Runs_Clean')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef2ec66",
   "metadata": {},
   "source": [
    "## Calculating Posterior Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5920da",
   "metadata": {},
   "source": [
    "Based on the post the prior distribution will have a mean of 39.15 and a standard deviation of 8.78. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b802450e",
   "metadata": {},
   "source": [
    "The formulas from the textbook are found below here. The new posterior mean and standard error are calculated using the original prior function with the new likelihood function. \n",
    "\n",
    "$$\\mu_n = \\frac{\\displaystyle \\frac{1}{\\tau_{0}^2}\\mu_0 + \\frac{n}{\\sigma^2}\\bar{y}}{\\displaystyle \\frac{1}{\\tau_{0}^2} + \\frac{n}{\\sigma^2}}$$\n",
    "$$$$\n",
    "$$\\frac{1}{\\tau_{n}^2} = \\frac{1}{\\tau_{0}^2}+ \\frac{n}{\\sigma^2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50cb4697",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the prior mean and standard deviations, and prior distribution, will also define a distribution size variable\n",
    "prior_mean = 39.15\n",
    "prior_std = 8.78\n",
    "dist_size = 100000\n",
    "prior = np.random.normal(prior_mean, prior_std, size = dist_size)\n",
    "# define the likelihood dataframe to store the relevant columns\n",
    "l_df = stats[[\"var\", \"Dismissal\",\"Avg\", \"std_err\"]]\n",
    "l_df[\"Avg\"] = l_df[\"Avg\"].astype('float64') # converting average column to float\n",
    "size = len(l_df)\n",
    "# define a likelihood array to contain the likelihood distribution\n",
    "l_array = np.zeros(shape = (size, dist_size))\n",
    "# define the posterior dataframe to store the posterior mean and standard deviations\n",
    "# need to know likelihood size to determine size of the empty dataframe\n",
    "pos_df = pd.DataFrame(np.nan,index = range(size), columns = [\"pos_mean\",\"pos_std\"])\n",
    "pos_array = np.zeros(shape = (size, dist_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec88601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a for loop to iterate over the different likelihood values\n",
    "\n",
    "for i in range(size):\n",
    "    l_var, l_n, l_avg, l_std_err = l_df.loc[i,\"var\"], l_df.loc[i,'Dismissal'], l_df.loc[i,'Avg'], l_df.loc[i,'std_err'] # assigning values from dataframe to own variables\n",
    "    l_array[i] = np.random.normal(l_avg, l_std_err, size = dist_size)\n",
    "    pos_df.loc[i, 'pos_mean'] = ((prior_mean / (prior_std) ** 2) + ((l_n * l_avg) / l_var)) / (prior_std ** -2 + (l_n / l_var)) # calculating the posterior mean\n",
    "    pos_df.loc[i, 'pos_std'] = (prior_std ** (-2) + (l_n / l_var)) ** (-0.5) # calculating the posterior standard error\n",
    "    pos_array[i] = np.random.normal(pos_df.loc[i, 'pos_mean'],pos_df.loc[i, 'pos_std'], size = dist_size) # calculating posterior distribution\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2271b6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_concat = pd.concat([pos_df, l_df, stats[['Runs_Clean']]], axis = 1) #concatenating the posterior and likelihood statistics into one dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e94555",
   "metadata": {},
   "source": [
    "## Visualising Posterior Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcebb5e7",
   "metadata": {},
   "source": [
    "Plotting the data using seaborn and matplotlib libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2201cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import animation\n",
    "%matplotlib qt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f482b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(1,1, figsize = (10,112))\n",
    "\n",
    "def animate(i):\n",
    "    ax1.cla()\n",
    "    ax1.set_xlim([0,200])\n",
    "    plt.xlabel(\"Runs\")\n",
    "    plt.ylabel(\"Relative Probability\")\n",
    "    sns.kdeplot(l_array[i], label = 'Likelihood', shade = True, ax = ax1)\n",
    "    sns.kdeplot(pos_array[i], label = 'Posterior', shade = True, ax = ax1)\n",
    "    sns.kdeplot(prior, label = 'Prior', shade = True, ax = ax1)\n",
    "    ax1.set(title = \"Dismissal: \" +str(df_concat.loc[i,\"Dismissal\"]))\n",
    "    ax1.legend([\"Likelihood\",\"Posterior\",\"Prior\"])\n",
    "    \n",
    "anim = animation.FuncAnimation(fig, animate, frames = len(l_array), interval = 500)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb70ed5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saves the animation as a gif\n",
    "# can comment it out if taking too long to run\n",
    "writergif = animation.PillowWriter(fps=5) \n",
    "anim.save('Sachin Tendulkar.gif', writer=writergif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68e4dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import arviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38b0b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "hpd = arviz.hdi(pos_array[-1], hdi_prob = 0.9)\n",
    "hpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12935c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "arviz.plot_posterior(pos_array[-1], hdi_prob = 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44bf4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"There is a 90% probability that the players average exists within {:.2f} and {:.2f}. This results in a bayesian average of {:.2f} with a standard error of {:.2f}\"\\\n",
    "     .format(round(hpd[0],2),round(hpd[1],2),round(pos_df['pos_mean'].iloc[-1],2), round(pos_df['pos_std'].iloc[-1],2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d92f57",
   "metadata": {},
   "source": [
    "As we can see Sachin Tendulkar's bayesian average is only slightly different compared to the classical average, 52.30 and 53.79 respectivley. Due to the large amount of innings he played coupled with the low variance in his scores the standard error is also low. This is also reflected in the small range of the 90% interval 47.61 and 56.84, showing how consistent he was throughout his career. The low end of the 90% interval, 47.61, shows that Tendulkar was much better than the average batter. However, to call him an all time great would need to compare him with more batters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d01b82c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
